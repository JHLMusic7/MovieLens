---
title: "Report on MovieLens"
author: "Jo Leung"
date: "28/12/2020"
output:
  pdf_document: default
  html_document: default
---

```{r create dataset, include=FALSE}
##########################################################
# Create edx set, validation set (final hold-out test set)
##########################################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(data.table)
library(lubridate)

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

# if using R 3.6 or earlier:
#movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
#                                           title = as.character(title),
#                                           genres = as.character(genres))
# if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))


movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)



```

## Introduction

This project showcases different techniques learnt from the HarvardX Data Science program. The objective of this project is to create a movie recommendation system using the MovieLens dataset, in particular the 10M version (for easier computation).

Different machine learning algorithms will be trained in one subset of the data to predict movie ratings in the validation set (which will NOT be used in the training process and is strictly used for evaluation only).

The edx set contains data that will be used for training and testing different machine learning algorithms.

The validation dataset is used for evaluation only, which will be used in this report to evaluate the accuracy of different algorithms.

## Analysis

### Data wrangling
```{r data wrangling, echo=FALSE}
head(edx)

```

We first begin by exploring the dataset through data visualization and data wrangling, from the above summary of the dataset, we can see that the timestamp (the year when the movie is rated) is not in an intuitive format thus we begin by converting the timestamp to year format.

```{r timestamp conversion, echo=FALSE}
#convert timestamp into date

edx <- edx %>% mutate(year_rated = year(as_datetime(timestamp)))
head(edx)

```

Furthermore, the year in which the movie is released is included in the title column, which we can extract using regex and separate function.

Now that we have access the the release year of the movie and the year in which the movie was rated, we can add in a column "age_when_rated" which indicates the age of the movie when the rating was performed to see if it affects the average rating.

```{r release date extraction, echo = FALSE}

edx <- edx %>% mutate(title = str_replace(title,"^(.+)\\s\\((\\d{4})\\)$","\\1__\\2" )) %>% 
  separate(title,c("title","year_released"),"__")

edx <- edx %>%
  mutate(age_when_rated = as.numeric(year_rated) - as.numeric(year_released))

edx <- edx %>%
  filter(year_released < 2018 & year_released > 1900) %>%
  filter(age_when_rated > 0)

head(edx)

#perform same data wrangling on validation dataset

validation <- validation %>% 
  mutate(year_rated = year(as_datetime(timestamp)))

validation <- validation %>% 
  mutate(title = str_replace(title,"^(.+)\\s\\((\\d{4})\\)$","\\1__\\2" )) %>% 
  separate(title,c("title","year_released"),"__") 

validation <- validation %>%
  mutate(age_when_rated = as.numeric(year_rated) - as.numeric(year_released))

validation <- validation %>%
  filter(year_released < 2018 & year_released > 1900) %>%
  filter(age_when_rated > 0)
```
### Data visualization

After the data wrangling process, we can now learn more about the dataset through data visualization.

We first explore the number of distinct movies and users in the dataset.

```{r distinct users and movies, echo = FALSE}

edx %>%
  summarize(n_users = n_distinct(userId),
            n_movies = n_distinct(movieId))
```

We can also look at the mean and standard deviation of the ratings

```{r mean and standard deviation, echo = FALSE}
mean(edx$rating)
sd(edx$rating)
```

We can make a table of the 10 most rated movies

```{r most rated movies, echo = FALSE}
#most rated movies
edx %>%
  group_by(title) %>%
  summarise(count = n(), title = title[1]) %>%
  top_n(10, count) %>%
  arrange(desc(count))
```

We can then look at how many ratings do users give to movies

```{r number of ratings per user, echo = FALSE}

#number of ratings per user
edx %>%
  group_by(userId) %>%
  summarise(n = n()) %>%
  ggplot(aes(n)) +
  geom_histogram(fill = "red", color = "black", bins = 10) +
  scale_x_log10()
```

From the plot, we can see that it kind of follows a normal distribution, with most users giving just under 100 ratings.

We can then look at the rating distribution.

```{r rating distribution, echo = FALSE}

#rating distributions
edx %>%
  group_by(rating) %>%
  ggplot(aes(rating)) +
  geom_bar(fill = "red", color = "black")
```

4 star is the most common rating given by user, another useful trend to observe is full star (1,2,3,4,5) ratings are more common than half star ratings (0.5, 1.5, 2.5, 3.5, 4.5)

Next, we look at genre distribution.

```{r genre distribution, echo = FALSE}

#genres distribution
edx %>%
  separate_rows(genres, sep = "\\|") %>%
  mutate(value = 1) %>%
  group_by(genres) %>%
  summarize(n = n()) %>%
  arrange(desc(n))
```

We can see from the table that drama is the most common genre, whilst 7 movies does not have a genre associated with it.

Finally, we look at the age of movie when it's rated against the mean rating to investigate whether there's a correlation, the mean rating overall is also plotted as a horizontal line.

```{r age vs mean rating, echo = FALSE}

#age of movie vs mean rating
edx %>%
  group_by(age_when_rated) %>%
  summarise(mean_age = mean(rating)) %>%
  ggplot(aes(age_when_rated, mean_age)) +
  geom_point() +
  geom_smooth(method = "loess", formula = y ~ x) +
  geom_line(aes(,mean(edx$rating)))
```

An interesting trend that we can observe from the trend is typically 10-15 years after a movie is released, its rating goes up above the mean rating, this is likely due to popular movies being rated more and spread more, causing them to be rated more and receive higher ratings due to their reputation over the years. This finding inspires the use of an age model later on to predict the rating of the movies. 

### Model evaluation

```{r training and testing dataset, include = FALSE}

#create training and test set using the edx set only

set.seed(20, sample.kind = "Rounding")

test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.2, list = FALSE)
training_set <- edx[-test_index,]
test_set <- edx[test_index,]

test_set <- test_set %>%
  semi_join(training_set, by = "movieId") %>%
  semi_join(training_set, by = "userId")

validation <- validation %>%
  semi_join(training_set, by = "movieId") %>%
  semi_join(training_set, by = "userId")

```

We will be using the RMSE (root mean squared error) as our evaluation benchmark, the target RMSE that we are aiming for is < 0.86490, we will see how different models performs at the end.

```{r RMSE, include = TRUE}

#define RMSE as our evaluation benchmark

RMSE <- function(true_rating, predicted_rating){
  sqrt(mean((true_rating - predicted_rating)^2))
}
```

The data is partitioned so 80% of the edx dataset is used for training and 20% of the edx dataset is used for testing.

The RMSE results presented in this report are results evaluated on the validation dataset which is NOT used for training in any way and it's only used for the evaluation presented here.

#### Model 1 | Mean Model

This is a naive approach that simply uses the mena rating to predict ratings for all movies.

```{r supress warining, include = FALSE}
# Suppress summarise info
options(dplyr.summarise.inform = FALSE)

```

```{r mean model, include = TRUE}

model_mean <- mean(training_set$rating)

rmse_mean <- RMSE(validation$rating, model_mean)
```

```{r mean model RMSE, echo = FALSE}

rmse_results <- tibble(method = "Mean model", RMSE = rmse_mean)

options(pillar.sigfig = 7)
rmse_results %>%
  arrange(desc(RMSE))
```

#### Model 2 | Age Model

This model uses the age of the movie when rated to predict rating (age bias)

```{r age model, include = TRUE}

model_age <- training_set %>%
  group_by(age_when_rated) %>%
  summarize(b_a = mean(rating - model_mean))

predicted_age <- model_mean + validation %>%
  left_join(model_age, by = 'age_when_rated') %>%
  pull(b_a)

rmse_age <- RMSE(validation$rating, predicted_age)
```

```{r age model RMSE, echo = FALSE}
rmse_results <- bind_rows(rmse_results, tibble(method = "Rating age model",
                                               RMSE = rmse_age))

rmse_results %>%
  arrange(desc(RMSE))
```

#### Model 3 | Movie Model

This model uses the movidId to predict rating (movie bias)

```{r movie model, include = TRUE}
model_movie <- training_set %>%
  group_by(movieId) %>%
  summarize(b_m = mean(rating - model_mean))

predicted_movie <- model_mean + validation %>%
  left_join(model_movie, by = 'movieId') %>%
  pull(b_m) 

rmse_movie <- RMSE(validation$rating, predicted_movie)
```

```{r movie model RMSE, echo = FALSE}
rmse_results <- bind_rows(rmse_results, tibble(method = "MovieID model",
                                               RMSE = rmse_movie))


rmse_results %>%
  arrange(desc(RMSE))
```

#### Model 4 | User Model

This model uses the userId to predict rating (user bias)

```{r user model, include = TRUE}
model_user <- training_set %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - model_mean))

predicted_user <- model_mean + validation %>%
  left_join(model_user, by = 'userId') %>%
  pull(b_u)

rmse_user <- RMSE(validation$rating, predicted_user)
```

```{r user model RMSE, echo = FALSE}
rmse_results <- bind_rows(rmse_results, tibble(method = "UserID model",
                                               RMSE = rmse_user))

rmse_results %>%
  arrange(desc(RMSE))
```

#### Model 5 | Genre Model

We then attempt to use the genre to predict rating (genre bias)

```{r genre model, include = TRUE}
model_genre <- training_set %>%
  group_by(genres) %>%
  summarize(b_g = mean(rating - model_mean))

predicted_genre <- model_mean + validation %>%
  left_join(model_genre, by = 'genres') %>%
  pull(b_g)

rmse_genre <- RMSE(validation$rating, predicted_genre)

```

```{r genre model RMSE, echo = FALSE}
rmse_results <- bind_rows(rmse_results, tibble(method = "Genre model",
                                               RMSE = rmse_genre))

rmse_results %>%
  arrange(desc(RMSE))
```

#### Model 6 | Release Date Model

We also attempt to use the movie's release date to predict rating (release date bias)

```{r release date model, include = TRUE}
model_release <- training_set %>%
  group_by(year_released) %>%
  summarize(b_r = mean(rating - model_mean))

predicted_release <- model_mean + validation %>%
  left_join(model_release, by = 'year_released') %>%
  pull(b_r)

rmse_release <- RMSE(validation$rating, predicted_release)

```

```{r release date model RMSE, echo = FALSE}
rmse_results <- bind_rows(rmse_results, tibble(method = "Release date model",
                                               RMSE = rmse_release))

rmse_results %>%
  arrange(desc(RMSE))
```

As shown from the table, when only considering one bias for prediction, Movie model has the lowest RMSE, followed by User Model.

The worst performing model is the mean model which is the simplest and naive so it is expected that the performance isnt't as good as the other models. 

We now move on to combine different bias to form more complex models.

#### Model 7 | Movie + User Model

We first consider using both MovieId and UserId to predict rating.

```{r movie + user model, include = TRUE}
user_effect <- training_set %>%
  left_join(model_movie, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - model_mean - b_m))

predicted_m_u <- validation %>%
  left_join(model_movie, by = 'movieId') %>%
  left_join(user_effect, by = 'userId') %>%
  mutate(pred = model_mean + b_m + b_u)

rmse_m_u <- RMSE(validation$rating, predicted_m_u$pred)

```

```{r movie + user model RMSE, echo = FALSE}
rmse_results <- bind_rows(rmse_results, tibble(method = "MovieID + UserID model",
                                               RMSE = rmse_m_u))

rmse_results %>%
  arrange(desc(RMSE))
```

#### Model 8 | Rating Age + Release Date Model

We then consider combining rating age and release date to predict rating.

```{r age + release date model, include = TRUE}
release_effect <- training_set %>%
  left_join(model_age, by='age_when_rated') %>%
  group_by(year_released) %>%
  summarize(b_r = mean(rating - model_mean - b_a))

predicted_a_r <- validation %>%
  left_join(model_age, by = 'age_when_rated') %>%
  left_join(release_effect, by = 'year_released') %>%
  mutate(pred = model_mean + b_a + b_r)

rmse_a_r <- RMSE(validation$rating, predicted_a_r$pred)

```

```{r age + release date model RMSE, echo = FALSE}
rmse_results <- bind_rows(rmse_results, tibble(method = "Rating Age + Release date model",
                                               RMSE = rmse_a_r))

rmse_results %>%
  arrange(desc(RMSE))
```

From the table, we can see that the movie + user model greatly improved the RMSE, whereas the age + release date model didn't help predict ratings.

We then consider using regularization to help improve our algorithm.

Regularization is the concept of constraining the total variability of the effect sizes by penalizing large estimates that come from small sample sizes.

#### Model 9 | Regularized approach using Movie + User

We will consider a reularized model using movieId and userid.

```{r regularized movie + user model, include = TRUE}
lambdas <- seq(0, 10, 0.25)
rmses <- sapply(lambdas, function(l){
  
  mean <- mean(training_set$rating)
  
  b_m <- training_set %>%
    group_by(movieId) %>%
    summarize(b_m = sum(rating - mean)/(n()+l))
  
  b_u <- training_set %>%
    left_join(b_m, by='movieId') %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - mean - b_m)/(n()+l))
  
  rpred_m_u <- validation %>%
    left_join(b_m, by='movieId') %>%
    left_join(b_u, by='userId') %>%
    mutate(pred = mean + b_m + b_u) %>%
    .$pred
  
  return(RMSE(validation$rating, rpred_m_u))
})

qplot(lambdas, rmses)

rmse_rm_u <- min(rmses)

```

```{r regularized movie + user model RMSE, echo = FALSE}
rmse_results <- bind_rows(rmse_results, tibble(method = "Regularized Movie + User Effect Model",
                                               RMSE = rmse_rm_u))

rmse_results %>%
  arrange(desc(RMSE))
```

The regularized movie + user model again improved our RMSE.

#### Model 10 | Final model: Regularized combined bias model

We will now construct our final regularized model using all the bias which consists of:

MovieID, UserID, Genre, Release date, Rating age

```{r final model, include = TRUE}
lambdas <- seq(0, 10, 0.25)
rmses <- sapply(lambdas, function(l){
  
  mean <- mean(training_set$rating)
  
  b_m <- training_set %>%
    group_by(movieId) %>%
    summarize(b_m = sum(rating - mean)/(n()+l))
  
  b_u <- training_set %>%
    left_join(b_m, by='movieId') %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - mean - b_m)/(n()+l))
  
  b_a <- training_set %>%
    left_join(b_m, by='movieId') %>%
    left_join(b_u, by='userId') %>%
    group_by(age_when_rated) %>%
    summarize(b_a = sum(rating - mean - b_m - b_u)/(n()+l))
  
  b_r <- training_set %>%
    left_join(b_m, by='movieId') %>%
    left_join(b_u, by='userId') %>%
    left_join(b_a, by='age_when_rated') %>%
    group_by(year_released) %>%
    summarize(b_r = sum(rating - mean - b_m - b_u - b_a)/(n()+l))
  
  b_g <- training_set %>%
    left_join(b_m, by='movieId') %>%
    left_join(b_u, by='userId') %>%
    left_join(b_a, by='age_when_rated') %>%
    left_join(b_r, by='year_released') %>%
    group_by(genres) %>%
    summarize(b_g = sum(rating - mean - b_m - b_u - b_a - b_r)/(n()+l))
  
  rpred_m_u <- validation %>%
    left_join(b_m, by='movieId') %>%
    left_join(b_u, by='userId') %>%
    left_join(b_a, by='age_when_rated') %>%
    left_join(b_r, by='year_released') %>%
    left_join(b_g, by='genres') %>%
    mutate(pred = mean + b_m + b_u + b_a + b_r + b_g) %>%
    .$pred
  
  return(RMSE(validation$rating, rpred_m_u))
})

qplot(lambdas, rmses)

rmse_rcombined <- min(rmses)

```

```{r final model RMSE, echo = FALSE}
rmse_results <- bind_rows(rmse_results, tibble(method = "Regularized combined bias Model",
                                               RMSE = rmse_rcombined))

rmse_results %>%
  arrange(desc(RMSE))
```

## Results

Here again are the final RMSE results using the different algorithms. Our final model is a regularized model using a combination of different bias to predict rating, and it gives use the lowest RMSE and achieved our aim of < 0.86490.

```{r results, echo = FALSE}

rmse_results %>%
  arrange(desc(RMSE))
```

## Conclusion

These algorithms are the fundamental of recommendation systems which is used widely to make specific recommendation to users, the goal is to reduce RMSE as much as possible as using a 5-star rating system, a RMSE > 1 is not a good prediction by any means.

Using the skills we learnt from the course, we are able to build a model that can achieve a RMSE of 0.8623960.